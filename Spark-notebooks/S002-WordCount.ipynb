{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc7d3744-9c6e-426c-a621-e2dcfb1506d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textFile: org.apache.spark.rdd.RDD[String] = hdfs://localhost:9000/words.txt MapPartitionsRDD[1] at textFile at <console>:25\n",
       "res2: Long = 6\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val textFile =sc.textFile(\"hdfs://localhost:9000/words.txt\")\n",
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f4a57d6-6788-4ba1-ba73-fd5029c117c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(\"   spark kafka  \", \" kafka   spark pyspark \", \"                \", spark, \"\", \"APACHE Kafka APache SParK \")\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb8cd80-c09f-43eb-92b9-2feed57dd4b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lowerCaseRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at <console>:26\n",
       "res4: Array[String] = Array(spark kafka, kafka   spark pyspark, \"\", spark, \"\", apache kafka apache spark)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lowerCaseRdd = textFile.map (line => line.trim().toLowerCase())\n",
    "lowerCaseRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f87626-5876-4380-8c58-97ebb25328c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordArrayRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[3] at map at <console>:26\n",
       "res5: Array[Array[String]] = Array(Array(spark, kafka), Array(kafka, \"\", \"\", spark, pyspark), Array(\"\"), Array(spark), Array(\"\"), Array(apache, kafka, apache, spark))\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordArrayRdd = lowerCaseRdd.map (line => line.split(\" \"))\n",
    "wordArrayRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa7e74b-3a1b-417d-8170-340573d41746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordRdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at flatMap at <console>:26\n",
       "res6: Array[String] = Array(spark, kafka, kafka, \"\", \"\", spark, pyspark, \"\", spark, \"\", apache, kafka, apache, spark)\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordRdd = wordArrayRdd.flatMap(arr => arr)\n",
    "wordRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4070fbd-fe28-456e-bf9b-a928d1a57df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordPairRdd: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[5] at map at <console>:26\n",
       "res7: Array[(String, Int)] = Array((spark,1), (kafka,1), (kafka,1), (\"\",1), (\"\",1), (spark,1), (pyspark,1), (\"\",1), (spark,1), (\"\",1), (apache,1), (kafka,1), (apache,1), (spark,1))\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordPairRdd = wordRdd.map (word => (word, 1))\n",
    "wordPairRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9aa4662c-19e8-46aa-8a3a-62257134ad0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordCountRdd: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[6] at reduceByKey at <console>:26\n",
       "res8: Array[(String, Int)] = Array((pyspark,1), (\"\",4), (apache,2), (kafka,3), (spark,4))\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCountRdd = wordPairRdd.reduceByKey ((acc, value) => acc +value )\n",
    "wordCountRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79f18b62-fa08-4583-a89c-2254e2eefd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCountRdd.saveAsTextFile(\"hdfs://localhost:9000/word-count-scala\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe435282-d0c1-4155-9e39-aafc4f47247b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
